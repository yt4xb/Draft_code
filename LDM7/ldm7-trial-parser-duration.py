#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""@package ldm_parser
Copyright (C) 2019 University of Virginia. All rights reserved.

file      ldm7-trial-parser.py
author    Yuanlong Tan <yt4xb@virginia.edu>
version	  1.0
date      Aug. 28, 2019
LICENSE

This program is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2 of the License, or（at your option）
any later version.

This program is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
more details at http://www.gnu.org/copyleft/gpl.html

brief     parses log files generated by LDM7 receivers.
usage     python ldm7-trial-parser.py <logfile> <csvfile-to-write>
"""


from __future__ import division
import re
import sys
import pytz
from datetime import datetime


def parseMLDM(line):
    """Parses the product size and elapsed time received by MLDM.

    Parses the product size and elapsed receiving time consumed
    for the product (which is received by MLDM) in the given line
    of log file.

    Args:
        line: A line of the raw log file.

    Returns:
        (-1, -1, -1, -1): If no valid size or time is found.
        (prodindex, prodsize, rxtime): A tuple of product index, product size,
	receiving time and FMTP retransmitted blocks.
    """
    match = re.search(r'.*Mldm.*Received', line)
    if match:
	split_line = line.split()
	prodindex = int(split_line[9].split(",")[0])
	# col 14 is size in bytes
	size = int(split_line[14])
	index = int(split_line[17])
	# col 0 is the arrival time, col 7 is the insertion time.
	arrival_time = split_line[0]
	rxtime = float(split_line[6])
	retrans = int(split_line[11].split(",")[0])
	return (prodindex, arrival_time, size, rxtime, retrans, index)
    else:
	return (-1, -1, -1, -1, -1, -1)

def parseBackstop(feedtype, line):
	"""Parses the product size and elapsed time received by the backstop.

	Parses the product size and elapsed receiving time consumed for the
	product (which is received by the backstop) in the given line of log file.

	Args:
		line: A line of the raw log file.

	Returns:
		(-1, -1, -1): If no valid size or time is found.
		(prodindex, prodsize, rxtime): A tuple of product index, product size
									   and receiving time.
	"""
	match = re.search(r'.*down7.*Inserted', line)
	if match:
		split_line = line.split()
		arrival_time = datetime.strptime(split_line[0], "%Y%m%dT%H%M%S.%fZ")
		insert_time  = datetime.strptime(split_line[6], "%Y%m%d%H%M%S.%f")
		# the last column is product index
		prodindex = int(split_line[8])
		# col 6 is size in bytes
		size = int(split_line[5])
		# col 0 is the arrival time, col 7 is the insertion time.
		# arrival_time = parse(split_line[0]).astimezone(pytz.utc).arrival_time.replace(tzinfo=None)
		rxtime = (arrival_time - insert_time).total_seconds()
		return (prodindex, size, rxtime)
	else:
		return (-1, -1, -1)


def extractLog(filename):
    """Extracts the key information from the log file.

    Args:
        filename: Filename of the log file.

    Returns:
        (complete_set, complete_dict, vset): extracted groups.
    """
    complete_set  = set()
    complete_dict = {}
    with open(filename, 'r') as logfile:
        for i, line in enumerate(logfile):
            (mprodid, arrival_time, msize, mrxtime, retrans, index) = parseMLDM(line)
            if mprodid >= 0:
                complete_set |= {mprodid}
                if not complete_dict.has_key(mprodid):
                    complete_dict[mprodid] = (arrival_time, msize, mrxtime, retrans, index)
    logfile.close()
    return (complete_set, complete_dict)


def main(logfile):
    """Reads the raw log file and parses it.

    Reads the raw ldmd log file, parses each line and computes throughput
    and VSR over an aggregate size.

    Args:
        logfile: Filename of the log file.
        csvfile : Filename of the new file to contain output results.
    """
    name = logfile.split(".")[0]
    csvfile = name+"-time.csv"
    w = open(csvfile, 'w+')
    nounce = 0
    first = 0
    last = 0
    timestamp_tmp = timestamp_end = "20210101T000000.120226Z"
    (rx_success_set, rx_success_dict) = extractLog(logfile)
    tmp_str = 'timestamp, mindex, prodindex, latency (s)' + '\n'
    w.write(tmp_str)
    for i in rx_success_set:
	if rx_success_dict[i][0] > timestamp_end and rx_success_dict[i][0] > timestamp_tmp:
		timestamp_end = rx_success_dict[i][0]
		last = rx_success_dict[i][4]
	if rx_success_dict[i][4] == 000:
		timestamp_init = rx_success_dict[i][0]
		nounce = rx_success_dict[i][2]
		first = rx_success_dict[i][4]
	tmp_str = str(rx_success_dict[i][0]) + ',' + str(rx_success_dict[i][4]) + ',' + str(i) + ',' + str(rx_success_dict[i][2]) + '\n'
	w.write(tmp_str)
	timestamp_tmp = rx_success_dict[i][0]
	previous_time = datetime.strptime(timestamp_end, "%Y%m%dT%H%M%S.%fZ")
    	later_time  = datetime.strptime(timestamp_tmp, "%Y%m%dT%H%M%S.%fZ")
	if later_time > previous_time:
		timestamp_end = timestamp_tmp
		last = rx_success_dict[i][4]

	
    w.close()
    begin_time = datetime.strptime(timestamp_init, "%Y%m%dT%H%M%S.%fZ")
    end_time  = datetime.strptime(timestamp_end, "%Y%m%dT%H%M%S.%fZ")
    total_time = (end_time - begin_time).total_seconds() + nounce
    aggregated_sizes = 8653636089
    average_thru = aggregated_sizes * 8 / total_time / 1000000
    #print str(timestamp_init) + '\t' + str(timestamp_end) + '\t' + str(total_time) +'\t' + str(nounce)
    #print str(first) + '\t' + str(last) + '\t' + str(total_time) + '\t' + str(average_thru)
    print str(average_thru)
if __name__ == "__main__":
    main(sys.argv[1])
